{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "groceries_classification.ipynb",
      "provenance": [],
      "mount_file_id": "1nrAIW0kv_14G9UFaBDDpVJrwHryAFJSC",
      "authorship_tag": "ABX9TyMFDSnIsDJDpjGcFDlzwQ/c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galihweb000haha/CNN/blob/main/groceries_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNnANmWy-TI9"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rianrajagede/groceries_classification.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n15KU8xCbjX",
        "outputId": "4570420f-99eb-4984-99a1-921295369cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'groceries_classification'...\n",
            "remote: Enumerating objects: 1142, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 1142 (delta 6), reused 16 (delta 3), pack-reused 1121\u001b[K\n",
            "Receiving objects: 100% (1142/1142), 199.16 MiB | 41.49 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -c https://github.com/rianrajagede/groceries_classification -O groceries_classification.zip\n",
        "! unzip groceries_classification.zip\n",
        "! rm groceries_classification.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGtjM4QXB6v4",
        "outputId": "9c5a792c-991b-4c2a-c6d1-8d6f18569a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-22 19:18:35--  https://github.com/rianrajagede/groceries_classification\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘groceries_classification.zip’\n",
            "\n",
            "groceries_classific     [ <=>                ] 154.21K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-03-22 19:18:36 (3.07 MB/s) - ‘groceries_classification.zip’ saved [157907]\n",
            "\n",
            "Archive:  groceries_classification.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of groceries_classification.zip or\n",
            "        groceries_classification.zip.zip, and cannot find groceries_classification.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import ToTensor\n",
        "train_dir = \"groceries_classification/train/\"\n",
        "test_dir = \"groceries_classification/test\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.CenterCrop(150),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.CenterCrop(150),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "train_img = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "test_img = datasets.ImageFolder(test_dir, transform=test_transform)"
      ],
      "metadata": {
        "id": "L1qtcSL-CSmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train_img type   :\",type(train_img)) # objek datasets\n",
        "print(\"train_img length :\",len(train_img)) # 940 data train\n",
        "print(\"test_img length :\",len(test_img)) # 120 data test\n",
        "print(\"train_img classes:\",train_img.classes) # nama kelas\n",
        "print(\"train_img[0] type:\",type(train_img[0])) # tuple\n",
        "print(\"train_img[0][0] s:\",train_img[0][0].size()) # tensor citra\n",
        "print(\"train_img[0][1]  :\",train_img[0][1]) # label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibrl3UdQF101",
        "outputId": "d437a520-06ac-4c34-9354-269582d7c67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_img type   : <class 'torchvision.datasets.folder.ImageFolder'>\n",
            "train_img length : 940\n",
            "test_img length : 120\n",
            "train_img classes: ['JUICE', 'MILK', 'SODA', 'VINEGAR', 'WATER']\n",
            "train_img[0] type: <class 'tuple'>\n",
            "train_img[0][0] s: torch.Size([3, 150, 150])\n",
            "train_img[0][1]  : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "trainloaders = torch.utils.data.DataLoader(train_img, batch_size=16, shuffle=True)\n",
        "testloaders = torch.utils.data.DataLoader(test_img, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "iUA7x_T_F-BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=4, stride=1, padding=0)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0)\n",
        "    self.fc1 = nn.Linear(39200, 512)\n",
        "    self.fc2 = nn.Linear(512, 5)\n",
        "    self.do = nn.Dropout()\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.do(F.relu(self.fc1(x)))\n",
        "    x = self.fc2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "mVvvige_Gp_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "epoch = 20\n",
        "\n",
        "model.train()\n",
        "for i in range(epoch):\n",
        "  total_loss = 0\n",
        "  total_sample = 0\n",
        "  total_correct = 0\n",
        "\n",
        "  for image, label in trainloaders:\n",
        "\n",
        "    out = model(image) # LANGKAH 1: forward propagation\n",
        "    loss = criterion(out, label) # LANGKAH 2: hitung error\n",
        "    optimizer.zero_grad() # LANGKAH 3: bersihkan nilai gradient sisa komputasi\n",
        "    loss.backward() # LANGKAH 4: backpropagation\n",
        "    optimizer.step() # LANGKAH 5: update bobot berdasar algoritma optimizer\n",
        "\n",
        "    total_loss += loss.item() # untuk merata2 eror\n",
        "    total_sample += len(label) # untuk merata2 eror dan akurasi\n",
        "    total_correct += torch.sum(torch.max(out,1)[1]==label).item()*1.0 # untuk merata2 eror akurasi\n",
        "    \n",
        "  print(\"epoch\", i, total_loss/total_sample, total_correct/total_sample) # menampilkan rata2 eror dan akurasi tiap epoch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VTEeqdvJ4s4",
        "outputId": "3604febc-e23f-434a-c5bd-5e9ca4264f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 0.03642831000875919 0.35425531914893615\n",
            "epoch 1 0.020057505876459974 0.48723404255319147\n",
            "epoch 2 0.019226529306553778 0.5159574468085106\n",
            "epoch 3 0.017362424350799398 0.5606382978723404\n",
            "epoch 4 0.015737651764078344 0.6148936170212767\n",
            "epoch 5 0.014968176598244525 0.6457446808510638\n",
            "epoch 6 0.013149521135269328 0.6936170212765957\n",
            "epoch 7 0.012163667983197152 0.7202127659574468\n",
            "epoch 8 0.010717109891962498 0.7510638297872341\n",
            "epoch 9 0.009900979412362931 0.774468085106383\n",
            "epoch 10 0.008516497719795147 0.7957446808510639\n",
            "epoch 11 0.007513738566256584 0.8223404255319149\n",
            "epoch 12 0.007487180638820567 0.8308510638297872\n",
            "epoch 13 0.006850036344629653 0.85\n",
            "epoch 14 0.005978196764245947 0.8670212765957447\n",
            "epoch 15 0.005256506705537755 0.8840425531914894\n",
            "epoch 16 0.005243601856079507 0.902127659574468\n",
            "epoch 17 0.003850463682666738 0.9212765957446809\n",
            "epoch 18 0.0036482975679509185 0.9212765957446809\n",
            "epoch 19 0.0034922234396985237 0.9308510638297872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_loss = 0\n",
        "total_sample = 0\n",
        "total_correct = 0\n",
        "\n",
        "for image, label in testloaders:\n",
        "  out = model(image)\n",
        "  loss = criterion(out, label)\n",
        "\n",
        "  total_loss += loss.item()\n",
        "  total_sample += len(label)\n",
        "  total_correct += torch.sum(torch.max(out, 1)[1] == label).item()*1.0\n",
        "print(\"test loss\", total_loss/total_sample)\n",
        "print(\"test accuracy\", total_correct/total_sample)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKSvzCztK0sZ",
        "outputId": "8266ac63-93b2-485f-cb19-4a40b8226541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss 0.03749907861153285\n",
            "test accuracy 0.6583333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gs4JNoHuQCRO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}